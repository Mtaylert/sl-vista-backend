{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting and downloading models\n",
    "\n",
    "_____\n",
    "### For the base ggml model in English, we can run:\n",
    "```bash \n",
    "./models/download-ggml-model.sh base.en\n",
    "```\n",
    "\n",
    "#### This downloads the model in the models folder, and we can use it with:\n",
    "```bash\n",
    "./main -m models/ggml-base.en.bin -f samples/jfk.wav\n",
    "```\n",
    "____\n",
    "### For [Quantized models](https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#quantization), we can run:\n",
    "```bash\n",
    "make quantize\n",
    "./quantize models/ggml-base.en.bin models/ggml-base.en-q5_0.bin q5_0\n",
    "```\n",
    "\n",
    "#### This downloads the model in the models folder, and we can use it with:\n",
    "```bash\n",
    "./main -m models/ggml-base.en-q5_0.bin samples/jfk.wav\n",
    "```\n",
    "____\n",
    "### For CoreML Models we can run:\n",
    "```bash\n",
    "./models/generate-coreml-model.sh base.en\n",
    "```\n",
    "\n",
    "Then run:\n",
    "```bash\n",
    "make clean\n",
    "WHISPER_COREML=1 make -j\n",
    "```\n",
    "\n",
    "#### This downloads the model in the models folder, and we can use it with:\n",
    "```bash\n",
    "./main -m models/ggml-base.en.bin -f samples/jfk.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available models (under models folder)\n",
    "- ggml-base.en-q5_0.bin\n",
    "- ggml-base.en.bin \n",
    "    * Note, this will be the coreML optimized one if you ran coreML steps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-base.en.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 1\n",
      "whisper_model_load: qntvr         = 0\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:    Metal total size =   147.37 MB\n",
      "whisper_model_load: model size    =  147.37 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   18.87 MB\n",
      "whisper_init_state: kv pad  size  =    3.15 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.26 MB\n",
      "whisper_init_state: compute buffer (encode) =  135.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.65 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | METAL = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n",
      "\n",
      "[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n",
      "\n",
      "\n",
      "whisper_print_timings:     load time =   120.39 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =     5.06 ms\n",
      "whisper_print_timings:   sample time =    34.71 ms /   131 runs (    0.26 ms per run)\n",
      "whisper_print_timings:   encode time =   114.04 ms /     1 runs (  114.04 ms per run)\n",
      "whisper_print_timings:   decode time =    16.61 ms /     2 runs (    8.30 ms per run)\n",
      "whisper_print_timings:   batchd time =   121.35 ms /   125 runs (    0.97 ms per run)\n",
      "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
      "whisper_print_timings:    total time =   429.61 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Base model\n",
    "!./main -m models/ggml-base.en.bin -f samples/jfk.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper_init_from_file_with_params_no_state: loading model from 'models/ggml-base.en-q5_0.bin'\n",
      "whisper_init_with_params_no_state: use gpu    = 1\n",
      "whisper_init_with_params_no_state: flash attn = 0\n",
      "whisper_init_with_params_no_state: gpu_device = 0\n",
      "whisper_init_with_params_no_state: dtw        = 0\n",
      "whisper_model_load: loading model\n",
      "whisper_model_load: n_vocab       = 51864\n",
      "whisper_model_load: n_audio_ctx   = 1500\n",
      "whisper_model_load: n_audio_state = 512\n",
      "whisper_model_load: n_audio_head  = 8\n",
      "whisper_model_load: n_audio_layer = 6\n",
      "whisper_model_load: n_text_ctx    = 448\n",
      "whisper_model_load: n_text_state  = 512\n",
      "whisper_model_load: n_text_head   = 8\n",
      "whisper_model_load: n_text_layer  = 6\n",
      "whisper_model_load: n_mels        = 80\n",
      "whisper_model_load: ftype         = 8\n",
      "whisper_model_load: qntvr         = 2\n",
      "whisper_model_load: type          = 2 (base)\n",
      "whisper_model_load: adding 1607 extra tokens\n",
      "whisper_model_load: n_langs       = 99\n",
      "whisper_model_load:    Metal total size =    54.71 MB\n",
      "whisper_model_load: model size    =   54.71 MB\n",
      "whisper_backend_init_gpu: using Metal backend\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 22906.50 MB\n",
      "whisper_backend_init: using BLAS backend\n",
      "whisper_init_state: kv self size  =   18.87 MB\n",
      "whisper_init_state: kv cross size =   18.87 MB\n",
      "whisper_init_state: kv pad  size  =    3.15 MB\n",
      "whisper_init_state: compute buffer (conv)   =   16.26 MB\n",
      "whisper_init_state: compute buffer (encode) =  135.01 MB\n",
      "whisper_init_state: compute buffer (cross)  =    4.65 MB\n",
      "whisper_init_state: compute buffer (decode) =   98.19 MB\n",
      "\n",
      "system_info: n_threads = 4 / 12 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | METAL = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | CUDA = 0 | COREML = 0 | OPENVINO = 0\n",
      "\n",
      "main: processing 'samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, 5 beams + best of 5, lang = en, task = transcribe, timestamps = 1 ...\n",
      "\n",
      "\n",
      "[00:00:00.000 --> 00:00:08.000]   And so, my fellow Americans, ask not what your country can do for you.\n",
      "[00:00:08.000 --> 00:00:11.000]   Ask what you can do for your country.\n",
      "\n",
      "\n",
      "whisper_print_timings:     load time =    96.35 ms\n",
      "whisper_print_timings:     fallbacks =   0 p /   0 h\n",
      "whisper_print_timings:      mel time =     5.22 ms\n",
      "whisper_print_timings:   sample time =    36.71 ms /   145 runs (    0.25 ms per run)\n",
      "whisper_print_timings:   encode time =    88.38 ms /     1 runs (   88.38 ms per run)\n",
      "whisper_print_timings:   decode time =     6.44 ms /     1 runs (    6.44 ms per run)\n",
      "whisper_print_timings:   batchd time =   140.21 ms /   140 runs (    1.00 ms per run)\n",
      "whisper_print_timings:   prompt time =     0.00 ms /     1 runs (    0.00 ms per run)\n",
      "whisper_print_timings:    total time =   389.73 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Test the quantized model\n",
    "!./main -m models/ggml-base.en-q5_0.bin samples/jfk.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [CoreML](https://github.com/ggerganov/whisper.cpp?tab=readme-ov-file#core-ml-support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ane_transformers\n",
    "# !pip install openai-whisper\n",
    "# !pip install coremltools\n",
    "# !pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version 2.3.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n",
      "100%|████████████████████████████████████████| 139M/139M [06:43<00:00, 360kiB/s]\n",
      "ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=512, n_audio_head=8, n_audio_layer=6, n_vocab=51864, n_text_ctx=448, n_text_state=512, n_text_head=8, n_text_layer=6)\n",
      "/Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp/models/convert-whisper-to-coreml.py:137: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x.shape[1:] == self.positional_embedding.shape[::-1], \"incorrect audio shape\"\n",
      "/Users/carlos.salas/Documents/vista_hackathon_2024/vista_hackathon_env/lib/python3.11/site-packages/ane_transformers/reference/layer_norm.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert inputs.size(1) == self.num_channels\n",
      "/Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp/models/convert-whisper-to-coreml.py:79: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scale = float(dim_per_head)**-0.5\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|▉| 829/830 [00:00<00:00, 9768.91 o\n",
      "Running MIL frontend_pytorch pipeline: 100%|█| 5/5 [00:00<00:00, 145.40 passes/s\n",
      "Running MIL default pipeline: 100%|████████| 78/78 [00:01<00:00, 53.57 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|█| 12/12 [00:00<00:00, 204.53 passe\n",
      "done converting\n",
      "xcrun: error: unable to find utility \"coremlc\", not a developer tool or in PATH\n",
      "mv: rename models/coreml-encoder-base.en.mlmodelc to models/ggml-base.en-encoder.mlmodelc: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# !./models/generate-coreml-model.sh base.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the coreML model\n",
    "!./main -m models/ggml-base.en-q5_0.bin samples/jfk.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import sounddevice as sd\n",
    "import subprocess\n",
    "import wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_to_txt(input_filename: str, output_filename: str):\n",
    "    print('Running whisper transcription...')\n",
    "    # Compose the command of all components\n",
    "    command = ['./main', '-f', input_filename, '-otxt', '-of', output_filename, '-np']\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(indata, frames, time, status):\n",
    "    # Raise for status if required\n",
    "    if status:\n",
    "        print(status)\n",
    "    \n",
    "    # Create a tempfile to save the audio to, with autodeletion\n",
    "    with tempfile.NamedTemporaryFile(delete=True, suffix='.wav', prefix='audio_', dir='.') as tmpfile:\n",
    "        # Save the 5 second audio to a .wav file\n",
    "        with wave.open(tmpfile.name, 'wb') as wav_file:\n",
    "            wav_file.setnchannels(1)  # Mono audio\n",
    "            wav_file.setsampwidth(2)  # 16-bit audio\n",
    "            wav_file.setframerate(16000)  # Sample rate\n",
    "            wav_file.writeframes(indata)\n",
    "        \n",
    "        # Prepare the output filename\n",
    "        output_filename = tmpfile.name.replace('.wav', '')\n",
    "        \n",
    "        # Transcribe the audio to text using our whisper.cpp wrapper\n",
    "        transcribe_to_txt(tmpfile.name, output_filename)\n",
    "\n",
    "        # Print the transcribed text\n",
    "        with open(output_filename + '.txt', 'r') as file:\n",
    "            print(file.read())\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        os.remove(output_filename + '.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seconds buffer size - breaks up the callback into 5 second chunks for processing\n",
    "buffer_size_seconds = 5\n",
    "samplerate = 16000\n",
    "buffer_size = buffer_size_seconds * samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Press Ctrl+C to stop.\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Running whisper transcription...\n",
      "\n",
      "Recording stopped.\n"
     ]
    }
   ],
   "source": [
    "# Start recording\n",
    "try:\n",
    "    # Start recording with a rolling 5-second buffer\n",
    "    with sd.InputStream(callback=callback, dtype='int16', channels=1, samplerate=16000, blocksize=buffer_size_seconds):\n",
    "        print(\"Recording... Press Ctrl+C to stop.\")\n",
    "        while True:\n",
    "            pass\n",
    "except KeyboardInterrupt:\n",
    "    print('Recording stopped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with speaker diarization\n",
    "\n",
    "### First we will pull the diarize model using this:\n",
    "```bash\n",
    "/models/download-ggml-model.sh small.en-tdrz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ggml model small.en-tdrz from 'https://huggingface.co/akashmjn/tinydiarize-whisper.cpp' ...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1228  100  1228    0     0  10828      0 --:--:-- --:--:-- --:--:-- 10867\n",
      "100  465M  100  465M    0     0  1531k      0  0:05:10  0:05:10 --:--:-- 1531kk      0  0:05:56  0:00:14  0:05:42 1413k      0  0:05:31  0:00:39  0:04:52 1217k:05:20  0:00:51  0:04:29 1632k2k      0  0:05:21  0:00:53  0:04:28 1578k0:04:25 1410k 0  0:05:17  0:01:02  0:04:15 1660k:17  0:01:15  0:04:02 1627k1495k      0  0:05:18  0:01:30  0:03:48 1485k  1499k      0  0:05:17  0:01:39  0:03:38 1625k  0:05:14  0:01:59  0:03:15 1670k0:02:08  0:03:08 1407k0     0  1513k      0  0:05:14  0:02:54  0:02:20 1527k0  0:05:14  0:02:56  0:02:18 1483k 1472k   0     0  1512k      0  0:05:14  0:03:22  0:01:52 1331k      0  0:05:14  0:03:23  0:01:51 1391k 0  1513k      0  0:05:14  0:03:24  0:01:50 1516k0     0  1514k      0  0:05:14  0:03:25  0:01:49 1634k2k      0  0:05:12  0:03:47  0:01:25 1625k0  1526k      0  0:05:12  0:03:52  0:01:20 1691k     0  0:05:12  0:04:08  0:01:04 1559kk      0  0:05:12  0:04:17  0:00:55 1522k    0  0:05:12  0:04:26  0:00:46 1523k521k      0  0:05:12  0:04:32  0:00:40 1426k 0:04:43  0:00:28 1656k\n",
      "Done! Model 'small.en-tdrz' saved in '/Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp/models/ggml-small.en-tdrz.bin'\n",
      "You can now use it like this:\n",
      "\n",
      "  $ ./main -m /Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp/models/ggml-small.en-tdrz.bin -f samples/jfk.wav\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./models/download-ggml-model.sh small.en-tdrz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./main -m /Users/carlos.salas/Documents/vista_hackathon_2024/whisper.cpp/models/ggml-small.en-tdrz.bin -f samples/jfk.wavgit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vista_hackathon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
